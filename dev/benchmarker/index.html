<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>AutoTM Artifact Workflow · AutoTM</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">AutoTM</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">AutoTM</a></li><li><a class="tocitem" href="../installation/">Installation</a></li><li class="is-active"><a class="tocitem" href>AutoTM Artifact Workflow</a><ul class="internal"><li><a class="tocitem" href="#PMM-Configuring-1LM-and-2LM-1"><span>PMM - Configuring 1LM and 2LM</span></a></li><li><a class="tocitem" href="#PMM-Conventional-Benchmarks-1"><span>PMM - Conventional Benchmarks</span></a></li><li><a class="tocitem" href="#PMM-Inception-Case-Study-1"><span>PMM - Inception Case Study</span></a></li><li><a class="tocitem" href="#PMM-Large-Networks-1"><span>PMM - Large Networks</span></a></li><li><a class="tocitem" href="#GPU-1"><span>GPU</span></a></li><li><a class="tocitem" href="#Experiment-Customization-1"><span>Experiment Customization</span></a></li><li><a class="tocitem" href="#Results-Data-Structure-1"><span>Results Data Structure</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>AutoTM Artifact Workflow</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>AutoTM Artifact Workflow</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/darchr/AutoTM/blob/master/docs/src/benchmarker.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="AutoTM-Artifact-Workflow-1"><a class="docs-heading-anchor" href="#AutoTM-Artifact-Workflow-1">AutoTM Artifact Workflow</a><a class="docs-heading-anchor-permalink" href="#AutoTM-Artifact-Workflow-1" title="Permalink"></a></h1><p>This section outlines how to run the experiments performed in the AutoTM paper and generate Figures 7 to 12 from the paper. The code to run these experiments lives in <code>$AUTOTM_HOME/experiments/Benchmarker</code>. Unless otherwise specified, all commands given below should be executed from this directory. Julia should be started with <code>julia --project</code>.</p><ul><li><a href="#AutoTM-Artifact-Workflow-1">AutoTM Artifact Workflow</a></li><ul><li><a href="#PMM-Configuring-1LM-and-2LM-1">PMM - Configuring 1LM and 2LM</a></li><ul><li><a href="#Switching-to-1LM-1">Switching to 1LM</a></li><li><a href="#Switching-to-2LM-1">Switching to 2LM</a></li></ul><li><a href="#PMM-Conventional-Benchmarks-1">PMM - Conventional Benchmarks</a></li><ul><li><a href="#Kernel-Profiling-1">Kernel Profiling</a></li><li><a href="#Running-Benchmarks-1">Running Benchmarks</a></li><li><a href="#Generating-Plots-1">Generating Plots</a></li><li><a href="#Test-Run-1">Test Run</a></li></ul><li><a href="#PMM-Inception-Case-Study-1">PMM - Inception Case Study</a></li><ul><li><a href="#Running-the-Experiment-1">Running the Experiment</a></li><li><a href="#Generating-Plots-2">Generating Plots</a></li></ul><li><a href="#PMM-Large-Networks-1">PMM - Large Networks</a></li><ul><li><a href="#Kernel-Profiling-2">Kernel Profiling</a></li><li><a href="#AutoTM-Data-1">AutoTM Data</a></li><li><a href="#LM-Data-1">2LM Data</a></li><li><a href="#Generating-Plots-3">Generating Plots</a></li></ul><li><a href="#GPU-1">GPU</a></li><ul><li><a href="#Preparation-1">Preparation</a></li><li><a href="#Profiling-1">Profiling</a></li><li><a href="#Running-Benchmarks-2">Running Benchmarks</a></li><li><a href="#Generating-Plots-4">Generating Plots</a></li></ul><li><a href="#Experiment-Customization-1">Experiment Customization</a></li><ul><li><a href="#CPU-Changing-the-Number-of-Threads.-1">CPU - Changing the Number of Threads.</a></li><li><a href="#CPU-Changing-DRAM-Limits-1">CPU - Changing DRAM Limits</a></li><li><a href="#GPU-Changing-DRAM-Limits-1">GPU - Changing DRAM Limits</a></li><li><a href="#CPU/GPU-New-Networks-1">CPU/GPU - New Networks</a></li></ul><li><a href="#Results-Data-Structure-1">Results Data Structure</a></li><ul><li><a href="#CPU-1">CPU</a></li><li><a href="#GPU-2">GPU</a></li></ul></ul></ul><h2 id="PMM-Configuring-1LM-and-2LM-1"><a class="docs-heading-anchor" href="#PMM-Configuring-1LM-and-2LM-1">PMM - Configuring 1LM and 2LM</a><a class="docs-heading-anchor-permalink" href="#PMM-Configuring-1LM-and-2LM-1" title="Permalink"></a></h2><p>Servers with Intel Optane DC can be configured to run in either 1LM/AppDirect mode, where reads and writes to PMM are managed manually, or 2LM/Memory Mode where PMM is accessed as main memory with DRAM as a transparent cache.</p><p>Most of the AutoTM code expects to run in 1LM mode with PMM mounted to <code>/mnt/public</code>. Scripts are provided in the <code>$AUTOTM_HOME/scripts</code> directory to aid in switching modes.</p><h3 id="Switching-to-1LM-1"><a class="docs-heading-anchor" href="#Switching-to-1LM-1">Switching to 1LM</a><a class="docs-heading-anchor-permalink" href="#Switching-to-1LM-1" title="Permalink"></a></h3><p>Reboot the system and select 1LM in the BIOS. After reboot, navigate to <code>$AUTOTM_HOME/scripts</code> and run</p><pre><code class="language-sh">sudo ./change_1lm.sh</code></pre><p>Reboot the system again. After the system comes online again, navigate back to <code>$AUTOTM_HOME/scripts</code> and run</p><pre><code class="language-sh">sudo ./setup_1lm.sh</code></pre><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>The script <code>setup_1lm.sh</code> will destroy all data in PMM namespace 1.0. <strong>DO NOT</strong> run this script if there is any data on there that must be preserved.</p></div></div><p>The setup script will create a new file system on the NVDIMMs on Socket 1 and perform a direct-access filesystem mount to <code>/mnt</code>.</p><h3 id="Switching-to-2LM-1"><a class="docs-heading-anchor" href="#Switching-to-2LM-1">Switching to 2LM</a><a class="docs-heading-anchor-permalink" href="#Switching-to-2LM-1" title="Permalink"></a></h3><p>Reboot the system and select 2LM in the BIOS. After reboot, navigate to <code>$AUTOTM_HOME/scripts</code> and run</p><pre><code class="language-sh">sudo ./change_2lm.sh</code></pre><p>Reboot the system again. That is all.</p><h2 id="PMM-Conventional-Benchmarks-1"><a class="docs-heading-anchor" href="#PMM-Conventional-Benchmarks-1">PMM - Conventional Benchmarks</a><a class="docs-heading-anchor-permalink" href="#PMM-Conventional-Benchmarks-1" title="Permalink"></a></h2><p>Make sure the system is in AppDirect mode and that <code>setup_1lm.sh</code> has been executed.</p><h3 id="Kernel-Profiling-1"><a class="docs-heading-anchor" href="#Kernel-Profiling-1">Kernel Profiling</a><a class="docs-heading-anchor-permalink" href="#Kernel-Profiling-1" title="Permalink"></a></h3><p>Kernel timing profiling must happen separately before the actual execution of benchmarks due to memory fragmentation.</p><p>To perform kernel profiling, run </p><pre><code class="language-julia">using Benchmarker, AutoTM
Benchmarker.kernel_profile(
    Benchmarker.conventional_functions(),
    [AutoTM.Optimizer.Static, AutoTM.Optimizer.Synchronous, AutoTM.Optimizer.Numa],
    Benchmarker.common_ratios(),
)</code></pre><p>Kernel profiling for all networks can take hours. Grab a cup of coffee and let AutoTM do its thing.</p><p>The serialized data structure for the cached kernel profiles lives in <code>$AUTOTM_HOME/data/caches</code>.</p><h3 id="Running-Benchmarks-1"><a class="docs-heading-anchor" href="#Running-Benchmarks-1">Running Benchmarks</a><a class="docs-heading-anchor-permalink" href="#Running-Benchmarks-1" title="Permalink"></a></h3><p>Reboot the system before running these benchmarks. Ensure the system is under light load for best results.</p><pre><code class="language-julia">using Benchmarker, AutoTM

optimizers = [
    AutoTM.Optimizer.Static,
    AutoTM.Optimizer.Synchronous,
    AutoTM.Optimizer.Numa
]

ratios = Benchmarker.common_ratios()

for fn in Benchmarker.conventional_functions()
    Benchmarker.run_conventional(fn, optimizers, ratios)
end</code></pre><p>Results for these runs will be stored to <code>$AUTOTM_HOME/experiments/Benchmarker/data/cpu</code></p><h3 id="Generating-Plots-1"><a class="docs-heading-anchor" href="#Generating-Plots-1">Generating Plots</a><a class="docs-heading-anchor-permalink" href="#Generating-Plots-1" title="Permalink"></a></h3><p>To generate Figures 7, 9, and 11 - run the following</p><pre><code class="language-julia">using Benchmarker

# Figure 7
Benchmarker.plot_speedup()

# Figure 9
Benchmarker.plot_costs()

# Figure 11
Benchmarker.plot_conventional_error()</code></pre><h3 id="Test-Run-1"><a class="docs-heading-anchor" href="#Test-Run-1">Test Run</a><a class="docs-heading-anchor-permalink" href="#Test-Run-1" title="Permalink"></a></h3><p>For verification purposes, a small Vgg19 network is included.</p><pre><code class="language-none">using Benchmarker, AutoTM
Benchmarker.kernel_profile(Benchmarker.test_vgg())
Benchmarker.run_conventional(
    Benchmarker.test_vgg(),
    [AutoTM.Optimizer.Static, AutoTM.Optimizer.Synchronous, AutoTM.Optimizer.Numa],
    Benchmarker.common_ratios(),
)

# Generate Plots
Benchmarker.plot_speedup(
    models = [Benchmarker.test_vgg()],
)

Benchmarker.plot_conventional_error(
    models = [Benchmarker.test_vgg()],
)

Benchmarker.plot_costs(
    pairs = [Benchmarker.test_vgg() =&gt; &quot;synchronous&quot;],
)</code></pre><h2 id="PMM-Inception-Case-Study-1"><a class="docs-heading-anchor" href="#PMM-Inception-Case-Study-1">PMM - Inception Case Study</a><a class="docs-heading-anchor-permalink" href="#PMM-Inception-Case-Study-1" title="Permalink"></a></h2><p>This experiment explores the sensitivity of the ILP formulation to PMM/DRAM ratios. Make sure the kernels are profiled prior to performing this experiment.</p><h3 id="Running-the-Experiment-1"><a class="docs-heading-anchor" href="#Running-the-Experiment-1">Running the Experiment</a><a class="docs-heading-anchor-permalink" href="#Running-the-Experiment-1" title="Permalink"></a></h3><p>The Inception Case study simply involves running the <code>conventional_inception()</code> workload for a large number of PMM to DRAM ratios.</p><pre><code class="language-julia">using Benchmarker
Benchmarker.inception_case_study()</code></pre><h3 id="Generating-Plots-2"><a class="docs-heading-anchor" href="#Generating-Plots-2">Generating Plots</a><a class="docs-heading-anchor-permalink" href="#Generating-Plots-2" title="Permalink"></a></h3><p>To generate Figure 10a, 10b, and 10c, run</p><pre><code class="language-julia">using Benchmarker
Benchmarker.inception_case_study_plots()</code></pre><h2 id="PMM-Large-Networks-1"><a class="docs-heading-anchor" href="#PMM-Large-Networks-1">PMM - Large Networks</a><a class="docs-heading-anchor-permalink" href="#PMM-Large-Networks-1" title="Permalink"></a></h2><p>This experiment compares AutoTM with the hardware managed 2LM. The workloads used for this experiment all used on the order of 650 GB of memory and so far exceed the size of local DRAM.</p><h3 id="Kernel-Profiling-2"><a class="docs-heading-anchor" href="#Kernel-Profiling-2">Kernel Profiling</a><a class="docs-heading-anchor-permalink" href="#Kernel-Profiling-2" title="Permalink"></a></h3><p>As with the conventional workloads, kernel profiling must be performed. The command given below will perform all profiling. Be warned that because of the large number of unique kernels in DenseNet, profiling can take about a day. Thus, you may want to just run a subset of the workloads.</p><pre><code class="language-julia">using Benchmarker

workloads = [
    Benchmarker.large_vgg(),
    Benchmarker.large_inception(),
    Benchmarker.large_resnet(),
    Benchmarker.large_densenet()
]

Benchmarker.kernel_profile(workloads)</code></pre><h3 id="AutoTM-Data-1"><a class="docs-heading-anchor" href="#AutoTM-Data-1">AutoTM Data</a><a class="docs-heading-anchor-permalink" href="#AutoTM-Data-1" title="Permalink"></a></h3><p>Due to the large size of these workloads, the system should be rebooted between each run to minimize memory fragmentation. It&#39;s not absolutely necessary, but can help with consistency.</p><pre><code class="language-julia">using Benchmarker, AutoTM

### Run each of the large workloads

# Vgg
Benchmarker.run_large(Benchmarker.large_vgg(), AutoTM.Optimizer.Static)
Benchmarker.run_large(Benchmarker.large_vgg(), AutoTM.Optimizer.Synchronous)

# Inception
Benchmarker.run_large(Benchmarker.large_inception(), AutoTM.Optimizer.Static)
Benchmarker.run_large(Benchmarker.large_inception(), AutoTM.Optimizer.Synchronous)

# Resnet
Benchmarker.run_large(Benchmarker.large_resnet(), AutoTM.Optimizer.Static)
Benchmarker.run_large(Benchmarker.large_resnet(), AutoTM.Optimizer.Synchronous)

# DenseNet
Benchmarker.run_large(Benchmarker.large_densenet(), AutoTM.Optimizer.Static)
Benchmarker.run_large(Benchmarker.large_densenet(), AutoTM.Optimizer.Synchronous)</code></pre><h3 id="LM-Data-1"><a class="docs-heading-anchor" href="#LM-Data-1">2LM Data</a><a class="docs-heading-anchor-permalink" href="#LM-Data-1" title="Permalink"></a></h3><p>Switch over the system to 2LM using the process outlined above. Once the system is in 2LM, run the following commands</p><pre><code class="language-julia">using Benchmarker

Benchmarker.run_2lm(Benchmarker.large_vgg())
Benchmarker.run_2lm(Benchmarker.large_inception())
Benchmarker.run_2lm(Benchmarker.large_resnet())
Benchmarker.run_2lm(Benchmarker.large_densenet())</code></pre><h3 id="Generating-Plots-3"><a class="docs-heading-anchor" href="#Generating-Plots-3">Generating Plots</a><a class="docs-heading-anchor-permalink" href="#Generating-Plots-3" title="Permalink"></a></h3><p>This generates Figure 8.</p><pre><code class="language-julia">using Benchmarker

Benchmarker.plot_large()</code></pre><h2 id="GPU-1"><a class="docs-heading-anchor" href="#GPU-1">GPU</a><a class="docs-heading-anchor-permalink" href="#GPU-1" title="Permalink"></a></h2><h3 id="Preparation-1"><a class="docs-heading-anchor" href="#Preparation-1">Preparation</a><a class="docs-heading-anchor-permalink" href="#Preparation-1" title="Permalink"></a></h3><p>To allow data to be moved to the host system, CUDA needs pinned memory. Make sure to run <code>ulimit -l</code> to allow unlimited pinned host memory before running.</p><p>Navigate to the Benchmarker directory</p><pre><code class="language-sh">cd $AUTOTM_HOME/experiments/Benchmarker</code></pre><p>Run a new Julia session</p><pre><code class="language-sh">julia --project</code></pre><p>In the Julia REPL, make sure all dependencies are installed</p><pre><code class="language-julia">julia&gt; ]

(Benchmarker) pkg&gt; instantiate</code></pre><h3 id="Profiling-1"><a class="docs-heading-anchor" href="#Profiling-1">Profiling</a><a class="docs-heading-anchor-permalink" href="#Profiling-1" title="Permalink"></a></h3><p>Because of memory overheads, the GPU experiments are split into two parts. The first part involves generating the kernel profile information. The second part is the actual running of the experiments themselves.</p><p>To generate the kernel profile data, perform the following sequence of commands in the <code>Benchmarker</code> directory</p><pre><code class="language-julia">using Benchmarker, AutoTM

Benchmarker.gpu_profile()</code></pre><p>when the system finishes profiling, exit the Julia session.</p><h3 id="Running-Benchmarks-2"><a class="docs-heading-anchor" href="#Running-Benchmarks-2">Running Benchmarks</a><a class="docs-heading-anchor-permalink" href="#Running-Benchmarks-2" title="Permalink"></a></h3><p>In a new Julia session, run the benchmarks with</p><pre><code class="language-julia">using Benchmarker, AutoTM

Benchmarker.gpu_benchmarks()</code></pre><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>There are some default variables set for the amount of GPU DRAM and for the overhead of the ngraph/CUDA runtimes. These are set to 11 GB and 1 GB respectively for a RTX 2080Ti. With a different GPU/CUDA version, these will need to be changed. For example, if your GPU has 6 GB of memory, these values may be set using</p><pre><code class="language-julia">using Benchmarker, AutoTM

Benchmarker.GPU_MAX_MEMORY[] = 6_000_000_000
Benchmarker.GPU_MEMORY_OVERHEAD[] = 1_000_000_000</code></pre><p>Memory overhead can be queried using <code>nvidia-smi</code></p></div></div><h3 id="Generating-Plots-4"><a class="docs-heading-anchor" href="#Generating-Plots-4">Generating Plots</a><a class="docs-heading-anchor-permalink" href="#Generating-Plots-4" title="Permalink"></a></h3><p>Following benchmark runs, the GPU performance plot (Figure 12) are simply generated using</p><pre><code class="language-julia">Benchmarker.gpu_performance_plot()</code></pre><h2 id="Experiment-Customization-1"><a class="docs-heading-anchor" href="#Experiment-Customization-1">Experiment Customization</a><a class="docs-heading-anchor-permalink" href="#Experiment-Customization-1" title="Permalink"></a></h2><p>There are a couple of ways to customize experiments.</p><h3 id="CPU-Changing-the-Number-of-Threads.-1"><a class="docs-heading-anchor" href="#CPU-Changing-the-Number-of-Threads.-1">CPU - Changing the Number of Threads.</a><a class="docs-heading-anchor-permalink" href="#CPU-Changing-the-Number-of-Threads.-1" title="Permalink"></a></h3><p>The CPU portion of the code defaults to using 24 threads on Socket 1 of a dual socket system (with sockets numbered as 0 and 1). This can be changed by calling</p><pre><code class="language-none">AutoTM.setup_affinities(; omp_num_threads = nthreads)</code></pre><p>By default, thread affinities are assigned one per physical core, essentially disabling hyperthreading. If you really want hyperthreading, the keyword argument <code>threads_per_core = 2</code> may be passed to <code>setup_affinities</code>.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>Calls to <code>setup_affinities</code> only work if called before the first run of the ngraph compiler. The LLVM compiler backing ngraph doesn&#39;t like changing the number of OMP threads for some reason.</p><p>Also don&#39;t do something crazy like <code>omp_num_threads = 1024</code> - I have no idea what will happen.</p></div></div><p>Kernel profiles are parameterized by number of threads so you don&#39;t have to worry about kernel profiles clobbering when changing the number of threads.</p><h3 id="CPU-Changing-DRAM-Limits-1"><a class="docs-heading-anchor" href="#CPU-Changing-DRAM-Limits-1">CPU - Changing DRAM Limits</a><a class="docs-heading-anchor-permalink" href="#CPU-Changing-DRAM-Limits-1" title="Permalink"></a></h3><p>Supporting different PMM to DRAM ratios is straight forward. When calling <a href="@ref PMM - Conventional Benchmarks"><code>run_conventional</code></a> entry function, custom ratios may be passed. These ratios are simply defined by Julia&#39;s native <code>Rational{Int}</code> type. For example, for a 16 to 1 PMM to DRAM ratio, simply pass <code>16 // 1</code>. The resulting call might look like</p><pre><code class="language-julia">Benchmarker.run_conventional(
    Benchmarker.test_vgg(),
    [AutoTM.Optimizer.Synchronous],
    16 // 1
)</code></pre><p>Furthermore, a hard DRAM limit can be passed by just passing in an <code>Int</code> number of bytes to the third argument.</p><h3 id="GPU-Changing-DRAM-Limits-1"><a class="docs-heading-anchor" href="#GPU-Changing-DRAM-Limits-1">GPU - Changing DRAM Limits</a><a class="docs-heading-anchor-permalink" href="#GPU-Changing-DRAM-Limits-1" title="Permalink"></a></h3><p>The GPU DRAM limits can be changed by changing the <code>GPU_MAX_MEMORY</code> and <code>GPU_MEMORY_OVERHEAD</code> variables as described in <a href="@ref">GPU</a>.</p><h3 id="CPU/GPU-New-Networks-1"><a class="docs-heading-anchor" href="#CPU/GPU-New-Networks-1">CPU/GPU - New Networks</a><a class="docs-heading-anchor-permalink" href="#CPU/GPU-New-Networks-1" title="Permalink"></a></h3><p>The code to create the benchmarked networks lives in <code>$AUTOTM_HOME/AutoTM/src/zoo</code>. Networks are modeled following Julia&#39;s <a href="https://github.com/FluxML/Flux.jl">Flux</a> machine learning library and are converted into ngraph computation graphs (with the help of <a href="https://github.com/hildebrandmw/nGraph.jl">nGraph.jl</a> and the mighty <a href="https://github.com/jrevels/Cassette.jl">Cassette</a>).</p><p>Custom networks can be defined externally and passed as an <code>AutoTM.Actualizer</code> to the functions <code>Benchmarker.run_conventional</code> or <code>Benchmarker.run_gpu</code>. A detailed example is given below.</p><p>Suppose we want to model a simple MLP.</p><pre><code class="language-julia">using Benchmarker, Flux, AutoTM, nGraph

# Define a function that returns a simple MLP wrapped up in an `Actualizer`.
function mlp(batchsize)
    # Define the network
    network = Flux.Chain(
        Dense(4096, 4096, Flux.relu),
        Dense(4096, 4096, Flux.relu),
        Dense(4096, 4096, Flux.relu),
        Dense(4096, 10, Flux.relu),
        softmax,
    ) 

    # Create input array
    X = randn(Float32, 4096, batchsize)

    # Create dummy one-hot input
    Y = zeros(Float32, 10, batchsize)
    for i in 1:batchsize
        Y[rand(1:10)] = one(eltype(Y))
    end

    # Compute the loss function.  
    loss(x, y) = Flux.crossentropy(network(x), y)
    return AutoTM.Actualizer(loss, X, Y; optimizer = nGraph.SGD(Float32(0.005)))
end

# This function can now be passed to Benchmarker.run_gpu
# If running with a batchsize of 16
Benchmarker.run_gpu(() -&gt; mlp(16))</code></pre><p>The results from the above will end up in <code>$AUTOTM_HOME/experiments/Benchmarker/data/gpu</code> with the name <code>unknown_network</code>. Results can be expected by deserializing the data</p><pre><code class="language-julia">using Serialization
data = deserialize(&quot;data/gpu/unknown_network_asynchronous_gpu_profile.jls&quot;);
display(first(data.runs))

# Roughly Expected Output
#   :bytes_async_moved_dram    =&gt; 2003336
#   :bytes_input_tensors       =&gt; 545962020
#   :predicted_runtime         =&gt; 0.41705
#   :pmem_alloc_size           =&gt; 0x000000000012b000
#   :num_async_move_nodes      =&gt; 32
#   :num_dram_async_move_nodes =&gt; 17
#   :move_time                 =&gt; 0.0
#   :dram_alloc_size           =&gt; 269910080
#   :num_input_tensors         =&gt; 107
#   :num_dram_move_nodes       =&gt; 17
#   :actual_runtime            =&gt; 0.00424737
#   :bytes_output_tensors      =&gt; 744613584
#   :bytes_async_moved_pmem    =&gt; 2002056
#   :num_dram_input_tensors    =&gt; 107
#   :tensor_size_map           =&gt; Dict(...)
#   :num_dram_output_tensors   =&gt; 72
#   :bytes_moved_pmem          =&gt; 2002056
#   :num_pmem_async_move_nodes =&gt; 15
#   :num_kernels               =&gt; 72
#   :bytes_async_moved         =&gt; 4005392
#   :bytes_moved               =&gt; 0
#   :dram_limit                =&gt; 8597
#   :bytes_dram_input_tensors  =&gt; 545962020
#   :bytes_dram_output_tensors =&gt; 744613584
#   :bytes_moved_dram          =&gt; 2003336
#   :num_move_nodes            =&gt; 0
#   :num_output_tensors        =&gt; 72
#   :num_pmem_move_nodes       =&gt; 15
#   :oracle_time               =&gt; 4140.0</code></pre><h2 id="Results-Data-Structure-1"><a class="docs-heading-anchor" href="#Results-Data-Structure-1">Results Data Structure</a><a class="docs-heading-anchor-permalink" href="#Results-Data-Structure-1" title="Permalink"></a></h2><p>For convenience of generating plots, result data from benchmarking runs is stores as a serialized Julia data structure. The details of that data structure are provided here. Results themselves can be found in either <code>$AUTOTM_HOME/experiments/Benchmarker/data/cpu</code> or <code>$AUTOTM_HOME/experiments/Benchmarker/data/gpu</code>.</p><p>The top level struct is a Julia NamedTuple with the following fields</p><ul><li><code>io_size</code>: The size in bytes of a network&#39;s input and output tensors.</li><li><code>default_alloc_size</code>: The number of bytes required by ngraph to run the graph natively.</li><li><code>gpu_managed_runtime</code>: The runtime of a network when using cudaMallocManaged (only applies to networks run on the GPU)</li><li><code>runs</code>: A vector containing result data for all benchmark runs for this particular workload.   The item that varies between runs is the amount of DRAM allowed.   The elements of this vector are of type <code>Dict{Symbol, Any}</code>.</li></ul><h3 id="CPU-1"><a class="docs-heading-anchor" href="#CPU-1">CPU</a><a class="docs-heading-anchor-permalink" href="#CPU-1" title="Permalink"></a></h3><p>For CPU workloads, the metrics recorded in the <code>runs</code> dictionaries are</p><ul><li><p><code>:creation_times</code>: Time spent creating the ILP formulation.   This is a vector which may have multiple elements if the ILP was run multiple times due to defragmentation.</p></li><li><p><code>:optimization_times</code>: Time spent solving the ILP.   Like <code>creation_times</code>, this may have multiple entries.</p></li><li><p><code>:predicted_runtime</code>: Runtime predicted by the ILP</p></li><li><p><code>:dram_limit</code>: The DRAM limit passed to the optimizer.</p></li><li><p><code>:tensor_size_map</code>: A dictionary mapping intermediate tensor names to their size in bytes.</p></li><li><p><code>:config_map</code>: A dictionary mapping ngraph nodes to their input and output configuration.</p></li><li><p><code>:ratio</code>: The ratio of PMM to DRAM.</p></li><li><p><code>:num_move_nodes</code>: The number of move nodes emitted.</p></li><li><p><code>:num_pmem_move_nodes</code>: The number of move nodes moving data from DRAM to PMM.</p></li><li><p><code>:num_dram_move_nodes</code>: The nubmer of move nodes moving data from PMM to DRAM.</p></li><li><p><code>:bytes_moved</code>: The total amount of data in bytes moved between memory pools.</p></li><li><p><code>:bytes_moved_pmem</code>: The number of bytes moved from DRAM to PMM.</p></li><li><p><code>:bytes_moved_dram</code>: The number of bytes moved from PMM to DRAM.</p></li><li><p><code>:num_async_move_nodes</code>: The number of asynchronous move nodes generated.</p></li><li><p><code>:num_pmem_async_move_nodes</code>: The number of asynchronous move nodes from DRAM to PMM.</p></li><li><p><code>:num_dram_async_move_nodes</code>: The number of asynchronous move nodes from PMM to DRAM.</p></li><li><p><code>:bytes_async_moved</code>: The total amount of data in bytes moved asynchronously.</p></li><li><p><code>:bytes_async_moved_pmem</code>: The amount of data in bytes moved asynchronously from DRAM to PMM.</p></li><li><p><code>:bytes_async_moved_dram</code>: The amount of data in bytes moved asynchronously from PMM to DRAM.</p></li><li><p><code>:num_kernels</code>: The number of ngraph nodes in the computation graph.</p></li><li><p><code>:num_input_tensors</code>: The total number of kernel inputs in the computation graph.</p></li><li><p><code>:num_output_tensors</code>: The total number of kernel outputs in the computation graph.</p></li><li><p><code>:num_dram_input_tensors</code>: The number of kernel inputs that are in DRAM.</p></li><li><p><code>:num_dram_output_tensors</code>: The number of kernel outputs that are in DRAM.</p></li><li><p><code>:bytes_input_tensors</code>: The total size of all kernel inputs.</p></li><li><p><code>:bytes_output_tensors</code>: The total size of all kernel outputs.</p></li><li><p><code>:bytes_dram_input_tensors</code>: The total size of all kernel inputs that are in DRAM.</p></li><li><p><code>:bytes_dram_output_tensors</code>: The total size of all kernel outputs that are in DRAM.</p></li><li><p><code>:dram_alloc_size</code>: The actual allocation size made by ngraph for DRAM.</p></li><li><p><code>:pmem_alloc_size</code>: The actual size of the PMM pool allocated by ngraph.</p></li><li><p><code>:move_time</code>: Estimate of time spent moving data.   Estimated based on the number of move nodes and the expected time for each move node.</p></li></ul><p>If the workload was run, the following fields will also be included</p><ul><li><code>:actual_runtime</code>: The actual measured runtime of the workload.</li><li><code>:kernel_times</code>: A dictionary mapping kernel names to their actual runtime.</li></ul><h3 id="GPU-2"><a class="docs-heading-anchor" href="#GPU-2">GPU</a><a class="docs-heading-anchor-permalink" href="#GPU-2" title="Permalink"></a></h3><p>The entries in the GPU dictionary are largely the same.  In the case of the GPU, the term <code>pmem</code> refers to host DRAM and <code>dram</code> refers to device DRAM. Additionally, the GPU data has the following entry:</p><ul><li><code>:oracle_time</code>: Predicted fastest runtime if all kernels with selectable implementations used their fastest implementation.</li></ul></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../installation/">« Installation</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Thursday 5 December 2019 00:35">Thursday 5 December 2019</span>. Using Julia version 1.3.0.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
