var documenterSearchIndex = {"docs":
[{"location":"benchmarker/#AutoTM-Artifact-Workflow-1","page":"AutoTM Artifact Workflow","title":"AutoTM Artifact Workflow","text":"","category":"section"},{"location":"benchmarker/#","page":"AutoTM Artifact Workflow","title":"AutoTM Artifact Workflow","text":"This section outlines how to run the experiments performed in the AutoTM paper and generate Figures 7 to 12 from the paper. The code to run these experiments lives in $AUTOTM_HOME/experiments/Benchmarker. Unless otherwise specified, all commands given below should be executed from this directory. Julia should be started with julia --project.","category":"page"},{"location":"benchmarker/#","page":"AutoTM Artifact Workflow","title":"AutoTM Artifact Workflow","text":"Pages = [\"benchmarker.md\"]\nDepth = 3","category":"page"},{"location":"benchmarker/#PMM-Configuring-1LM-and-2LM-1","page":"AutoTM Artifact Workflow","title":"PMM - Configuring 1LM and 2LM","text":"","category":"section"},{"location":"benchmarker/#","page":"AutoTM Artifact Workflow","title":"AutoTM Artifact Workflow","text":"Servers with Intel Optane DC can be configured to run in either 1LM/AppDirect mode, where reads and writes to PMM are managed manually, or 2LM/Memory Mode where PMM is accessed as main memory with DRAM as a transparent cache.","category":"page"},{"location":"benchmarker/#","page":"AutoTM Artifact Workflow","title":"AutoTM Artifact Workflow","text":"Most of the AutoTM code expects to run in 1LM mode with PMM mounted to /mnt/public. Scripts are provided in the $AUTOTM_HOME/scripts directory to aid in switching modes.","category":"page"},{"location":"benchmarker/#Switching-to-1LM-1","page":"AutoTM Artifact Workflow","title":"Switching to 1LM","text":"","category":"section"},{"location":"benchmarker/#","page":"AutoTM Artifact Workflow","title":"AutoTM Artifact Workflow","text":"Reboot the system and select 1LM in the BIOS. After reboot, navigate to $AUTOTM_HOME/scripts and run","category":"page"},{"location":"benchmarker/#","page":"AutoTM Artifact Workflow","title":"AutoTM Artifact Workflow","text":"sudo ./change_1lm.sh","category":"page"},{"location":"benchmarker/#","page":"AutoTM Artifact Workflow","title":"AutoTM Artifact Workflow","text":"Reboot the system again. After the system comes online again, navigate back to $AUTOTM_HOME/scripts and run","category":"page"},{"location":"benchmarker/#","page":"AutoTM Artifact Workflow","title":"AutoTM Artifact Workflow","text":"sudo ./setup_1lm.sh","category":"page"},{"location":"benchmarker/#","page":"AutoTM Artifact Workflow","title":"AutoTM Artifact Workflow","text":"note: Note\nThe script setup_1lm.sh will destroy all data in PMM namespace 1.0. DO NOT run this script if there is any data on there that must be preserved.","category":"page"},{"location":"benchmarker/#","page":"AutoTM Artifact Workflow","title":"AutoTM Artifact Workflow","text":"The setup script will create a new file system on the NVDIMMs on Socket 1 and perform a direct-access filesystem mount to /mnt.","category":"page"},{"location":"benchmarker/#Switching-to-2LM-1","page":"AutoTM Artifact Workflow","title":"Switching to 2LM","text":"","category":"section"},{"location":"benchmarker/#","page":"AutoTM Artifact Workflow","title":"AutoTM Artifact Workflow","text":"Reboot the system and select 2LM in the BIOS. After reboot, navigate to $AUTOTM_HOME/scripts and run","category":"page"},{"location":"benchmarker/#","page":"AutoTM Artifact Workflow","title":"AutoTM Artifact Workflow","text":"sudo ./change_2lm.sh","category":"page"},{"location":"benchmarker/#","page":"AutoTM Artifact Workflow","title":"AutoTM Artifact Workflow","text":"Reboot the system again. That is all.","category":"page"},{"location":"benchmarker/#PMM-Conventional-Benchmarks-1","page":"AutoTM Artifact Workflow","title":"PMM - Conventional Benchmarks","text":"","category":"section"},{"location":"benchmarker/#","page":"AutoTM Artifact Workflow","title":"AutoTM Artifact Workflow","text":"Make sure the system is in AppDirect mode and that setup_1lm.sh has been executed.","category":"page"},{"location":"benchmarker/#Kernel-Profiling-1","page":"AutoTM Artifact Workflow","title":"Kernel Profiling","text":"","category":"section"},{"location":"benchmarker/#","page":"AutoTM Artifact Workflow","title":"AutoTM Artifact Workflow","text":"Kernel timing profiling must happen separately before the actual execution of benchmarks due to memory fragmentation.","category":"page"},{"location":"benchmarker/#","page":"AutoTM Artifact Workflow","title":"AutoTM Artifact Workflow","text":"To perform kernel profiling, run ","category":"page"},{"location":"benchmarker/#","page":"AutoTM Artifact Workflow","title":"AutoTM Artifact Workflow","text":"using Benchmarker, AutoTM\nBenchmarker.kernel_profile(\n    Benchmarker.conventional_functions(),\n    [AutoTM.Optimizer.Static, AutoTM.Optimizer.Synchronous, AutoTM.Optimizer.Numa],\n    Benchmarker.common_ratios(),\n)","category":"page"},{"location":"benchmarker/#","page":"AutoTM Artifact Workflow","title":"AutoTM Artifact Workflow","text":"Kernel profiling for all networks can take hours. Grab a cup of coffee and let AutoTM do its thing.","category":"page"},{"location":"benchmarker/#","page":"AutoTM Artifact Workflow","title":"AutoTM Artifact Workflow","text":"The serialized data structure for the cached kernel profiles lives in $AUTOTM_HOME/data/caches.","category":"page"},{"location":"benchmarker/#Running-Benchmarks-1","page":"AutoTM Artifact Workflow","title":"Running Benchmarks","text":"","category":"section"},{"location":"benchmarker/#","page":"AutoTM Artifact Workflow","title":"AutoTM Artifact Workflow","text":"Reboot the system before running these benchmarks. Ensure the system is under light load for best results.","category":"page"},{"location":"benchmarker/#","page":"AutoTM Artifact Workflow","title":"AutoTM Artifact Workflow","text":"using Benchmarker, AutoTM\n\noptimizers = [\n    AutoTM.Optimizer.Static,\n    AutoTM.Optimizer.Synchronous,\n    AutoTM.Optimizer.Numa\n]\n\nratios = Benchmarker.common_ratios()\n\nfor fn in Benchmarker.conventional_functions()\n    Benchmarker.run_conventional(fn, optimizers, ratios)\nend","category":"page"},{"location":"benchmarker/#","page":"AutoTM Artifact Workflow","title":"AutoTM Artifact Workflow","text":"Results for these runs will be stored to $AUTOTM_HOME/experiments/Benchmarker/data/cpu","category":"page"},{"location":"benchmarker/#Generating-Plots-1","page":"AutoTM Artifact Workflow","title":"Generating Plots","text":"","category":"section"},{"location":"benchmarker/#","page":"AutoTM Artifact Workflow","title":"AutoTM Artifact Workflow","text":"To generate Figures 7, 9, and 11 - run the following","category":"page"},{"location":"benchmarker/#","page":"AutoTM Artifact Workflow","title":"AutoTM Artifact Workflow","text":"using Benchmarker\n\n# Figure 7\nBenchmarker.plot_speedup()\n\n# Figure 9\nBenchmarker.plot_costs()\n\n# Figure 11\nBenchmarker.plot_conventional_error()","category":"page"},{"location":"benchmarker/#Test-Run-1","page":"AutoTM Artifact Workflow","title":"Test Run","text":"","category":"section"},{"location":"benchmarker/#","page":"AutoTM Artifact Workflow","title":"AutoTM Artifact Workflow","text":"For verification purposes, a small Vgg19 network is included.","category":"page"},{"location":"benchmarker/#","page":"AutoTM Artifact Workflow","title":"AutoTM Artifact Workflow","text":"using Benchmarker, AutoTM\nBenchmarker.kernel_profile(Benchmarker.test_vgg())\nBenchmarker.run_conventional(\n    Benchmarker.test_vgg(),\n    [AutoTM.Optimizer.Static, AutoTM.Optimizer.Synchronous, AutoTM.Optimizer.Numa],\n    Benchmarker.common_ratios(),\n)\n\n# Generate Plots\nBenchmarker.plot_speedup(\n    models = [Benchmarker.test_vgg()],\n)\n\nBenchmarker.plot_conventional_error(\n    models = [Benchmarker.test_vgg()],\n)\n\nBenchmarker.plot_costs(\n    pairs = [Benchmarker.test_vgg() => \"synchronous\"],\n)","category":"page"},{"location":"benchmarker/#PMM-Inception-Case-Study-1","page":"AutoTM Artifact Workflow","title":"PMM - Inception Case Study","text":"","category":"section"},{"location":"benchmarker/#","page":"AutoTM Artifact Workflow","title":"AutoTM Artifact Workflow","text":"This experiment explores the sensitivity of the ILP formulation to PMM/DRAM ratios. Make sure the kernels are profiled prior to performing this experiment.","category":"page"},{"location":"benchmarker/#Running-the-Experiment-1","page":"AutoTM Artifact Workflow","title":"Running the Experiment","text":"","category":"section"},{"location":"benchmarker/#","page":"AutoTM Artifact Workflow","title":"AutoTM Artifact Workflow","text":"The Inception Case study simply involves running the conventional_inception() workload for a large number of PMM to DRAM ratios.","category":"page"},{"location":"benchmarker/#","page":"AutoTM Artifact Workflow","title":"AutoTM Artifact Workflow","text":"using Benchmarker\nBenchmarker.inception_case_study()","category":"page"},{"location":"benchmarker/#Generating-Plots-2","page":"AutoTM Artifact Workflow","title":"Generating Plots","text":"","category":"section"},{"location":"benchmarker/#","page":"AutoTM Artifact Workflow","title":"AutoTM Artifact Workflow","text":"To generate Figure 10a, 10b, and 10c, run","category":"page"},{"location":"benchmarker/#","page":"AutoTM Artifact Workflow","title":"AutoTM Artifact Workflow","text":"using Benchmarker\nBenchmarker.inception_case_study_plots()","category":"page"},{"location":"benchmarker/#PMM-Large-Networks-1","page":"AutoTM Artifact Workflow","title":"PMM - Large Networks","text":"","category":"section"},{"location":"benchmarker/#","page":"AutoTM Artifact Workflow","title":"AutoTM Artifact Workflow","text":"This experiment compares AutoTM with the hardware managed 2LM. The workloads used for this experiment all used on the order of 650 GB of memory and so far exceed the size of local DRAM.","category":"page"},{"location":"benchmarker/#Kernel-Profiling-2","page":"AutoTM Artifact Workflow","title":"Kernel Profiling","text":"","category":"section"},{"location":"benchmarker/#","page":"AutoTM Artifact Workflow","title":"AutoTM Artifact Workflow","text":"As with the conventional workloads, kernel profiling must be performed. The command given below will perform all profiling. Be warned that because of the large number of unique kernels in DenseNet, profiling can take about a day. Thus, you may want to just run a subset of the workloads.","category":"page"},{"location":"benchmarker/#","page":"AutoTM Artifact Workflow","title":"AutoTM Artifact Workflow","text":"using Benchmarker\n\nworkloads = [\n    Benchmarker.large_vgg(),\n    Benchmarker.large_inception(),\n    Benchmarker.large_resnet(),\n    Benchmarker.large_densenet()\n]\n\nBenchmarker.kernel_profile(workloads)","category":"page"},{"location":"benchmarker/#AutoTM-Data-1","page":"AutoTM Artifact Workflow","title":"AutoTM Data","text":"","category":"section"},{"location":"benchmarker/#","page":"AutoTM Artifact Workflow","title":"AutoTM Artifact Workflow","text":"Due to the large size of these workloads, the system should be rebooted between each run to minimize memory fragmentation. It's not absolutely necessary, but can help with consistency.","category":"page"},{"location":"benchmarker/#","page":"AutoTM Artifact Workflow","title":"AutoTM Artifact Workflow","text":"using Benchmarker, AutoTM\n\n### Run each of the large workloads\n\n# Vgg\nBenchmarker.run_large(Benchmarker.large_vgg(), AutoTM.Optimizer.Static)\nBenchmarker.run_large(Benchmarker.large_vgg(), AutoTM.Optimizer.Synchronous)\n\n# Inception\nBenchmarker.run_large(Benchmarker.large_inception(), AutoTM.Optimizer.Static)\nBenchmarker.run_large(Benchmarker.large_inception(), AutoTM.Optimizer.Synchronous)\n\n# Resnet\nBenchmarker.run_large(Benchmarker.large_resnet(), AutoTM.Optimizer.Static)\nBenchmarker.run_large(Benchmarker.large_resnet(), AutoTM.Optimizer.Synchronous)\n\n# DenseNet\nBenchmarker.run_large(Benchmarker.large_densenet(), AutoTM.Optimizer.Static)\nBenchmarker.run_large(Benchmarker.large_densenet(), AutoTM.Optimizer.Synchronous)","category":"page"},{"location":"benchmarker/#LM-Data-1","page":"AutoTM Artifact Workflow","title":"2LM Data","text":"","category":"section"},{"location":"benchmarker/#","page":"AutoTM Artifact Workflow","title":"AutoTM Artifact Workflow","text":"Switch over the system to 2LM using the process outlined above. Once the system is in 2LM, run the following commands","category":"page"},{"location":"benchmarker/#","page":"AutoTM Artifact Workflow","title":"AutoTM Artifact Workflow","text":"using Benchmarker\n\nBenchmarker.run_2lm(Benchmarker.large_vgg())\nBenchmarker.run_2lm(Benchmarker.large_inception())\nBenchmarker.run_2lm(Benchmarker.large_resnet())\nBenchmarker.run_2lm(Benchmarker.large_densenet())","category":"page"},{"location":"benchmarker/#Generating-Plots-3","page":"AutoTM Artifact Workflow","title":"Generating Plots","text":"","category":"section"},{"location":"benchmarker/#","page":"AutoTM Artifact Workflow","title":"AutoTM Artifact Workflow","text":"This generates Figure 8.","category":"page"},{"location":"benchmarker/#","page":"AutoTM Artifact Workflow","title":"AutoTM Artifact Workflow","text":"using Benchmarker\n\nBenchmarker.plot_large()","category":"page"},{"location":"benchmarker/#GPU-1","page":"AutoTM Artifact Workflow","title":"GPU","text":"","category":"section"},{"location":"benchmarker/#Preparation-1","page":"AutoTM Artifact Workflow","title":"Preparation","text":"","category":"section"},{"location":"benchmarker/#","page":"AutoTM Artifact Workflow","title":"AutoTM Artifact Workflow","text":"Navigate to the Benchmarker directory","category":"page"},{"location":"benchmarker/#","page":"AutoTM Artifact Workflow","title":"AutoTM Artifact Workflow","text":"cd $AUTOTM_HOME/experiments/Benchmarker","category":"page"},{"location":"benchmarker/#","page":"AutoTM Artifact Workflow","title":"AutoTM Artifact Workflow","text":"Run a new Julia session","category":"page"},{"location":"benchmarker/#","page":"AutoTM Artifact Workflow","title":"AutoTM Artifact Workflow","text":"julia --project","category":"page"},{"location":"benchmarker/#","page":"AutoTM Artifact Workflow","title":"AutoTM Artifact Workflow","text":"In the Julia REPL, make sure all dependencies are installed","category":"page"},{"location":"benchmarker/#","page":"AutoTM Artifact Workflow","title":"AutoTM Artifact Workflow","text":"julia> ]\n\n(Benchmarker) pkg> instantiate","category":"page"},{"location":"benchmarker/#Profiling-1","page":"AutoTM Artifact Workflow","title":"Profiling","text":"","category":"section"},{"location":"benchmarker/#","page":"AutoTM Artifact Workflow","title":"AutoTM Artifact Workflow","text":"Because of memory overheads, the GPU experiments are split into two parts. The first part involves generating the kernel profile information. The second part is the actual running of the experiments themselves.","category":"page"},{"location":"benchmarker/#","page":"AutoTM Artifact Workflow","title":"AutoTM Artifact Workflow","text":"To generate the kernel profile data, perform the following sequence of commands in the Benchmarker directory","category":"page"},{"location":"benchmarker/#","page":"AutoTM Artifact Workflow","title":"AutoTM Artifact Workflow","text":"using Benchmarker, AutoTM\n\nBenchmarker.gpu_profile()","category":"page"},{"location":"benchmarker/#","page":"AutoTM Artifact Workflow","title":"AutoTM Artifact Workflow","text":"when the system finishes profiling, exit the Julia session.","category":"page"},{"location":"benchmarker/#Running-Benchmarks-2","page":"AutoTM Artifact Workflow","title":"Running Benchmarks","text":"","category":"section"},{"location":"benchmarker/#","page":"AutoTM Artifact Workflow","title":"AutoTM Artifact Workflow","text":"In a new Julia session, run the benchmarks with","category":"page"},{"location":"benchmarker/#","page":"AutoTM Artifact Workflow","title":"AutoTM Artifact Workflow","text":"using Benchmarker, AutoTM\n\nBenchmarker.gpu_benchmarks()","category":"page"},{"location":"benchmarker/#","page":"AutoTM Artifact Workflow","title":"AutoTM Artifact Workflow","text":"note: Note\nThere are some default variables set for the amount of GPU DRAM and for the overhead of the ngraph/CUDA runtimes. These are set to 11 GB and 1 GB respectively for a RTX 2080Ti. With a different GPU/CUDA version, these will need to be changed. For example, if your GPU has 6 GB of memory, these values may be set usingusing Benchmarker, AutoTM\n\nBenchmarker.GPU_MAX_MEMORY[] = 6_000_000_000\nBenchmarker.GPU_MEMORY_OVERHEAD[] = 1_000_000_000Memory overhead can be queried using nvidia-smi","category":"page"},{"location":"benchmarker/#Generating-Plots-4","page":"AutoTM Artifact Workflow","title":"Generating Plots","text":"","category":"section"},{"location":"benchmarker/#","page":"AutoTM Artifact Workflow","title":"AutoTM Artifact Workflow","text":"Following benchmark runs, the GPU performance plot (Figure 12) are simply generated using","category":"page"},{"location":"benchmarker/#","page":"AutoTM Artifact Workflow","title":"AutoTM Artifact Workflow","text":"Benchmarker.gpu_performance_plot()","category":"page"},{"location":"installation/#Installation-1","page":"Installation","title":"Installation","text":"","category":"section"},{"location":"installation/#","page":"Installation","title":"Installation","text":"Clone the repository with","category":"page"},{"location":"installation/#","page":"Installation","title":"Installation","text":"git clone --recursive https://github.com/darchr/AutoTM\nexport AUTOTM_HOME=$(pwd)/AutoTM","category":"page"},{"location":"installation/#Setup-1","page":"Installation","title":"Setup","text":"","category":"section"},{"location":"installation/#","page":"Installation","title":"Installation","text":"A simple setup needs to be performed to indicate how the project will be used. To enter the setup, run","category":"page"},{"location":"installation/#","page":"Installation","title":"Installation","text":"cd $AUTOTM_HOME\njulia --color=yes setup.jl","category":"page"},{"location":"installation/#","page":"Installation","title":"Installation","text":"The following selections can be made - choose which are appropriate for your system:","category":"page"},{"location":"installation/#","page":"Installation","title":"Installation","text":"Use NVDIMMs in 1LM (requires a Cascade Lake system with Optane DC NVDIMMs)\nUse of a GPU (requires CUDA 10.1 or CUDA 10.2)\nUse Gurobi as the ILP solver (requires a Gurobi license (see below)).   If Gurobi is not selected, the open source Cbc solver will be used.   Please note that the original experiments were run with Gurobi.","category":"page"},{"location":"installation/#Building-1","page":"Installation","title":"Building","text":"","category":"section"},{"location":"installation/#","page":"Installation","title":"Installation","text":"Launch Julia from the AutoTM project","category":"page"},{"location":"installation/#","page":"Installation","title":"Installation","text":"cd $AUTOTM_HOME/AutoTM\njulia --project","category":"page"},{"location":"installation/#","page":"Installation","title":"Installation","text":"In the Julia REPL, press ] to switch to package (pkg) mode and run following commands:","category":"page"},{"location":"installation/#","page":"Installation","title":"Installation","text":"julia> ]\n(AutoTM) pkg> instantiate\n(AutoTM) pkg> build -v","category":"page"},{"location":"installation/#","page":"Installation","title":"Installation","text":"This will trigger the build process for our custom version of ngraph. Passing the -v command to build will helpfully display any errors that occur during the build process.","category":"page"},{"location":"installation/#Using-the-Gurobi-ILP-solver-(optional)-1","page":"Installation","title":"Using the Gurobi ILP solver (optional)","text":"","category":"section"},{"location":"installation/#","page":"Installation","title":"Installation","text":"The results in the AutoTM paper use Gurobi for the ILP solver. However, Gurobi requires a license to run. Free trial and academic licenses are available from the Gurobi website: https://www.gurobi.com","category":"page"},{"location":"installation/#","page":"Installation","title":"Installation","text":"If using Gurobi, please obtain a license and install the software according the instructions on the website.","category":"page"},{"location":"installation/#","page":"Installation","title":"Installation","text":"Then, when building the project, make sure to run","category":"page"},{"location":"installation/#","page":"Installation","title":"Installation","text":"julia> ENV[\"GUROBI_HOME\"] = \"path/to/gurobi\"","category":"page"},{"location":"installation/#","page":"Installation","title":"Installation","text":"in Julia before executing the build step above.","category":"page"},{"location":"installation/#","page":"Installation","title":"Installation","text":"note: Note\nUsing the Gurobi ILP solver is optional. If not selected during the setup step, an open-source solver Cbc will be used.","category":"page"},{"location":"#AutoTM-1","page":"AutoTM","title":"AutoTM","text":"","category":"section"},{"location":"#","page":"AutoTM","title":"AutoTM","text":"Documentation for the AutoTM source code and experiments/benchmarks.","category":"page"}]
}
